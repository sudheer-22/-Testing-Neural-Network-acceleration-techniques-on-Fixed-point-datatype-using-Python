# Testing NeuralNetwork acceleration techniques on Fixedpoint datatype using Python
A small neural network containing a single layer(of 4 neurons) which takes 16-bit thermometer code as input and gives binary output corresponding to it.
8 bit fixedpoint datatye has been developed has been simulated in python by writing respective functions. Bit width of the fractional and interger parts has been been varied and checked. Least no of epochs were required when 2bits and 5bits for integer and fractional parts respectively and 1bit for sign. Stochastic rounding method has aslo been tested. It is found that task is achieved only when the range for both integral and fractional parts have been assigned a little more than that of the word size. Same methods has been tried on a three layer fully connected neural network that trains on MNIST data set. But due to complexity issues it is impractical to complete training using these methods.   
